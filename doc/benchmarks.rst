Benchmarks
==========

Image generated by bm_chaos (took 3 sec on CPython 3.5):

.. image:: images/bm_chaos.png
   :alt: Chaos game, bm_chaos benchmark

::

    python3 performance/benchmarks/bm_chaos.py --worker -l1 -w0 -n1 --filename chaos.ppm --width=512 --height=512 --iterations 50000

Image generated by bm_raytrace (took 68.4 sec on CPython 3.5):

.. image:: images/bm_raytrace.jpg
   :alt: Pure Python raytracer

::

    python3 performance/benchmarks/bm_raytrace.py --worker --filename=raytrace.ppm  -l1 -w0 -n1 -v --width=800 --height=600


Available Groups
----------------

Like individual benchmarks (see "Available benchmarks" below), benchmarks group
are allowed after the `-b` option. Use ``python3 -m performance list_groups``
to list groups and their benchmarks.

Available benchmark groups:

* ``2n3``: Benchmarks compatible with both Python 2 and Python 3
* ``all``: Group including all benchmarks
* ``apps``: "High-level" applicative benchmarks (2to3, Chameleon, Tornado HTTP)
* ``calls``: Microbenchmarks on function and method calls
* ``default``: Group of benchmarks run by default by the ``run`` command
* ``etree``: XML ElementTree
* ``math``: Float and integers
* ``regex``: Collection of regular expression benchmarks
* ``serialize``: Benchmarks on ``pickle`` and ``json`` modules
* ``startup``: Collection of microbenchmarks focused on Python interpreter
  start-up time.
* ``template``: Templating libraries

There is also a disabled ``threading`` group: collection of microbenchmarks for
Python's threading support. These benchmarks come in pairs: an iterative
version (iterative_foo), and a multithreaded version (threaded_foo).


Available Benchmarks
--------------------

- ``2to3`` - have the 2to3 tool translate itself.
- ``call_method`` - positional arguments-only method calls.
- ``call_method_slots`` - method calls on classes that use __slots__.
- ``call_method_unknown`` - method calls where the receiver cannot be predicted.
- ``call_simple`` - positional arguments-only function calls.
- ``chameleon`` - render a template using the ``chameleon`` module
- ``chaos`` - create chaosgame-like fractals
- ``crypto_pyaes`` - benchmark a pure-Python implementation of the AES
  block-cipher in CTR mode using the pyaes module.
- ``deltablue`` - DeltaBlue benchmark
- ``django_template`` - use the Django template system to build a 150x150-cell
  HTML table (``django.template`` module).
- ``dulwich_log``: Iterate on commits of the asyncio Git repository using
  the Dulwich module
- ``fannkuch``
- ``fastpickle`` - use the cPickle module to pickle a variety of datasets.
- ``fastunpickle`` - use the cPickle module to unnpickle a variety of datasets.
- ``float`` - artificial, floating point-heavy benchmark originally used
  by Factor.
- ``genshi``: Benchmark the ``genshi.template`` module

  * ``genshi_text``: Render template to plain text
  * ``genshi_xml``: Render template to XML

- ``go``: Go board game
- ``hexiom`` - Solver of Hexiom board game (level 25 by default)
- ``hg_startup`` - Get Mercurial's help screen.
- ``html5lib`` - parse the HTML 5 spec using html5lib.
- ``json_dumps`` - Benchmark ``json.dumps()``
- ``json_loads`` - Benchmark ``json.loads()``
- ``logging`` - Benchmarks on the ``logging`` module

  * ``logging_format``: Benchmark ``logger.warn(fmt, str)``
  * ``logging_simple``: Benchmark ``logger.warn(msg)``
  * ``logging_silent``: Benchmark ``logger.warn(msg)`` when the message is
    ignored

- ``mako`` - use the Mako template system to build a 150x150-cell HTML table.
- ``mdp`` - battle with damages and topological sorting of nodes in a graph
- ``meteor_contest`` - solver for Meteor Puzzle board
- ``nbody`` - the N-body Shootout benchmark. Microbenchmark for floating point
  operations.
- ``normal_startup`` - Measure the Python startup time
- ``nqueens`` - Simple, brute-force N-Queens solver
- ``pathlib`` - Test the performance of operations of the ``pathlib`` module.
  This benchmark stresses the creation of small objects, globbing, and system
  calls.
- ``pickle_dict`` - microbenchmark; use the cPickle module to pickle a lot of dicts.
- ``pickle_list`` - microbenchmark; use the cPickle module to pickle a lot of lists.
- ``pickle_pure_python`` - use the pure-Python pickle module to pickle a
  variety of datasets.
- ``pidigits`` - Calculating 2,000 digits of Ï€.  This benchmark stresses
  big integer arithmetic.
- ``pybench`` - run the standard Python PyBench benchmark suite. This is
  considered an unreliable, unrepresentative benchmark; do not base decisions
  off it. It is included only for completeness.
- ``pyflate`` - Pyflate benchmark: tar/bzip2 decompressor in pure Python
- ``raytrace`` - Simple raytracer.
- ``regex_compile`` - stress the performance of Python's regex compiler,
  rather than the regex execution speed.
- ``regex_dna`` - regex DNA benchmark using "fasta" to generate the test case
- ``regex_effbot`` - some of the original benchmarks used to tune mainline
  Python's current regex engine.
- ``regex_v8`` - Python port of V8's regex benchmark.
- ``richards`` - the classic Richards benchmark.
- ``scimark``:

  * ``scimark_sor`` - scimark: `Successive over-relaxation (SOR)
    <https://en.wikipedia.org/wiki/Successive_over-relaxation>`_ benchmark
  * ``scimark_sparse_mat_mult`` - scimark: `sparse matrix
    <https://en.wikipedia.org/wiki/Sparse_matrix>`_ `multiplication
    <https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm>`_ benchmark
  * ``scimark_monte_carlo`` - scimark: benchmark on the `Monte Carlo algorithm
    <https://en.wikipedia.org/wiki/Monte_Carlo_algorithm>`_ to compute the area
    of a disc
  * ``scimark_lu`` - scimark: `LU decomposition
    <https://en.wikipedia.org/wiki/LU_decomposition>`_ benchmark
  * ``scimark_fft`` - scimark: `Fast Fourier transform (FFT)
    <https://en.wikipedia.org/wiki/Fast_Fourier_transform>`_ benchmark

- ``spambayes`` - run a canned mailbox through a SpamBayes ham/spam classifier.
- ``spectral_norm`` - MathWorld: "Hundred-Dollar, Hundred-Digit Challenge
  Problems", Challenge #3.
- ``sqlalchemy_declarative`` - SQLAlchemy Declarative benchmark using SQLite
- ``sqlalchemy_imperative`` - SQLAlchemy Imperative benchmark using SQLite
- ``sqlite_synth`` - Benchmark Python aggregate for SQLite
- ``startup_nosite`` - Measure the Python startup time without importing
  the ``site`` module (``python -S``)
- ``sympy`` - Benchmark on the ``sympy`` module

  * ``sympy_expand``: Benchmark ``sympy.expand()``
  * ``sympy_integrate``: Benchmark ``sympy.integrate()``
  * ``sympy_str``: Benchmark ``str(sympy.expand())``
  * ``sympy_sum``: Benchmark ``sympy.summation()``

- ``telco`` - Benchmark the ``decimal`` module
- ``tornado_http`` - Benchmark HTTP server of the ``tornado`` module
- ``unpack_sequence`` - microbenchmark for unpacking lists and tuples.
- ``unpickle_list``
- ``unpickle_pure_python`` - use the pure-Python pickle module to unpickle a
  variety of datasets.
- ``xml_etree``: Benchmark the ``xml.etree`` module

  - ``xml_etree_generate``: Create an XML document
  - ``xml_etree_iterparse``: Benchmark ``etree.iterparse()``
  - ``xml_etree_parse``: Benchmark ``etree.parse()``
  - ``xml_etree_process``: Process an XML document
